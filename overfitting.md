## 1 什么是overfitting？
一个假设：机器学习中，假设训练数据集 与 验证集和测试集 是独立同分布(independently and identically distributed)的，使用已有数据进行模型训练，然后将得到的模型用于新的数据上，进行预测和推测。
**但是**，现实的数据不可能总是满足独立同分布，有时候我们现有的数据会非常少，不足以对整个数据集进行分布估计，导致在训练过程中，出现过拟合问题。一种情况是：随着模型复杂度的增加，拟合后的误差是逐渐缩小的，但是，随着训练的继续，模型复杂度继续增加，模型在验证集上的误差反而越来越大。这种情况便是过拟合，即随着模型复杂度的增加，拟合后的误差在训练集上减小，但其他数据集上却并未减小，甚至变得越来越大。
>一种更为直观的关于过拟合的解释是：在对训练数据进行拟合时，如果要求模型照顾到每个点，这将使得拟合函数波动性非常大，即**方差大**。在某些小区间里，函数值的变化性很剧烈，意味着函数在某些小区间里的导数值的绝对值非常大，由于自变量的值在给定的训练数据集中的一定的，因此只有系数足够大，才能保证导数的绝对值足够大。这种特性使我们能够使用正则化来削弱噪声和异常点的权重，以达到优化模型的目的。
## 2 四大主要解决方法
### 2.1 early stopping
核心思想是：在出现过拟合之前，停止迭代。使用**迭代次数截断**的方法来防止或拟合，即模型对训练数据集进行迭代收敛之前，就停止迭代。在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。
### 2.2 regularization
在优化代价函数/目标函数时，在函数后加一个正则项，来防止或拟合。
+ L1正则

基于L1范数，在目标函数后面加上参数的L1范数和项，即参数绝对值和与参数的积项，即：
$$ J(w) = L(w)+P(w) = L(w)+ \frac{\lambda }{n}\sum_{w}|w|$$

其中，λ 是正则项系数，权衡正则项与J(w)项的比重。参数w的更新公式为：
$$ w := w + \alpha \frac{\partial J(w)}{\partial w}+\beta \frac{\lambda }{n}sgn(w)$$s
在线性回归中（L1正则线性回归即为**Lasso回归**），常数项b的更新方程不包括正则项，即：
$$w := w + \alpha \frac{\partial J(w)}{\partial w}$$
当w为正时，更新后w会变小；当w为负时，更新后w会变大；因此L1正则项是为了使得那些原先处于零（即|w|≈0）附近的参数w往零移动，使得部分参数为零，从而降低模型的复杂度（模型的复杂度由参数决定），从而防止过拟合，提高模型的泛化能力。 其中，L1正则中有个问题，便是L1范数在0处不可导，即|w|在0处不可导，因此在w为0时，使用原来的未经正则化的更新方程来对w进行更新，即令sgn(0)=0，这样即： 
$$ sgn(w)|_{w>0}=1;sgn(w)|_{w<0}=-1;sgn(w)|_{w=0}=0 $$

+ L2正则

基于L2范数，即在目标函数后面加上参数的L2范数和项，即参数的平方和与参数的积项，即：
$$ J(w) = L(w)+P(w) = L(w)+ \frac{\lambda }{2n}\sum_{w}w^{2} $$
参数w的更新公式为：
$$  w := w + \alpha \frac{\partial J(w)}{\partial w}+\beta \frac{\lambda }{n}w $$
the formula of ridge regression in linear regression
$$  w := w + \alpha \frac{\partial J(w)}{\partial w} $$
L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。 在对模型参数进行更新学习的时候，有2种更新方式：mini-batch 和 full-batch。使用mini-batch时，数据被分成几部分，依次进行更新，当所有样本数据都更新完毕后，最后一次次epoch的损失函数值则为所有mini batch的平均损失值。
在full-batch中，每一次epoch，使用全部的训练样本进行更新，每次的损失函数值即为全部样本的误差之和。
+ 总结

正则化的目的是**为了降低模型的复杂度**，从而避免模型去过分拟合训练数据集，包括噪声和异常点。正则化是假设模型参数服从先验概率，即为模型参数添加先验，**不同的正则化方式的先验分布不一样**。添加了先验分布，就规定了参数的分布，使得模型的复杂度降低（试想一下，限定条件多了，是不是模型的复杂度降低了呢），这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。从贝叶斯学派的角度来看：数据量少的时候，先验知识可以防止过拟合；从频率学派角度看，正则项限定了参数的取值，从而提高了模型的稳定性，而稳定性强的模型不会过拟合，即控制模型空间。
> L1 和 L2的区别

L1正则是拉普拉斯先验，而L2正则则是高斯先验，但它们都服从均值为0，协方差为1/λ。当λ=0时，即没有先验，没有正则项，等同于先验分布具有无穷大的协方差，此时先验约束会非常弱，模型为了拟合所有的训练集数据， 参数w可以变得任意大从而使得模型不稳定，即**方差大而偏差小**。λ越大，表明先验分布协方差越小，偏差越大，模型越稳定。即，**加入正则项是在偏差bias与方差variance之间做平衡tradeoff**

### 2.3 dropout
在神经网络中，通过**修改神经网络结构本身**来避免过拟合。假设现在有一个三层的神经网络-input layer，hidden layer and output layer，在数据的训练过程中，我们随机删除隐藏层的若干神经元，即认为这些神经元不存在，同时保证输入层和输出层的神经元个数不变，然后使用BP算法对剩下的神经元构成的neural network进行参数学习，得到输出后，完成一次迭代。同样的在下一次迭代过程中，同样随机删除一些神经元，然后学习参数，输出结果。照这样继续迭代，直到训练结束。
### 2.4 data augmentation
使用越多的训练数据得到的模型，其在测试集上将获得更高的精确度。有几种方法可以扩增训练数据集：
```
复制已有数据，为其增加噪声
重采样
根据当前数据集估计数据分布参数，使用参数分布生成数据
```


[Reference](https://blog.csdn.net/heyongluoyao8/article/details/49429629)
[正态分布的前世今生](http://emma.memect.com/t/05400bfdb821bac2abba403afc4ad2e9d2e7d8ea845aa7227f2433ffbe7a9684/intro-normal-distribution.pdf)
