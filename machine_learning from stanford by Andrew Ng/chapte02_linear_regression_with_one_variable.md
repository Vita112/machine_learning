
## 1. Model Representation
+ reading materials
  我们使用$x^{(i)}$代表输入变量，也称特征变量；使用$y^{(i)}$代表输出变量，每一对（$x^{(i)}$，$y^{(i)}$）是一个训练样本。同时，使用X代表输入变量的空间，Y代表输出变量的空间，本例中，X=Y=R。<br>
  监督学习方法中，我们给出一个训练集，来学习一个函数h：X→Y，使得h(x)可以很好的预测出y的值。<br>
  上述过程，我们可以更形象化（pictorially）地来看：
```
training set → learning algorithm → function h<br>
input:x → function → output：predicated y
```

当预测的目标变量是连续的，我们称这是一个回归学习问题；当y只取一些分散的值时，我们称之为分类问题。


+ 1.1 一个引例-预测房价

假设我们有一个数据集，数据集包含某市的住房价格。使用这个数据集，我们画出一个 关于不同住房面积和与之对应的房价的 关系图。
现在你的一个朋友有一个面积为a平米的房子想要出售，我们要做的是：如何预测朋友的房子将会卖多少钱?<br>
*通过监督学习算法， 我们可以使用数据集建立一个模型，根据模型来预测朋友房子的售价。*

+ 1.2 线性回归

```
上节课我们知道，有2种常见的监督学习方式：回归和分类。
回归指的是，我们根据之前的数据预测出一个`准确的`输出值。
分类问题中，由于输入值对应的输出值离散地分布在数据空间中，并形成明显的某几类，因而对于我们想要进行预测的输入值，我们不会得到
准确的某个值，而是得到属于哪一类的结果。

```
我们使用预测房价问题说明监督学习中的线性回归问题。例子中某市的房价数据被称为训练集（a training example），包含不同房屋的面积及其价格。
我们使用`m表示训练样本的数目`，即训练集中有多少对房屋面积及其价格的数据；接着使用$x^{(i)}$代表`输入变量`，$y^{(i)}$代表`输出变量`,一对（$x^{(i)}$，$y^{(i)}$）。
代表一个训练样本。由于对于每一个输入值$x^{(i)}$来说，数据集都给出了准确的输出值$y^{(i)}$，通过观察X到Y某种映射关系（使用学习算法学习），我们得到一个有关x，y的函数h（h=hypothesis）。到此通过监督学习，我们可以说建立起了一个关于房屋面积和价格的模型，使用这个模型，可以得出给定输入值对应的输出值。<br>
这个模型也被成为**线性回归模型（Linear Regression Model）:**
$$j(\theta _{0}, \theta _{1})=\frac{1}{2m}\sum_{i=1}^{m}(\hat{y_{i}}-y_{i})^{2}$$
由于模型中只有一个变量x，又被称为`单变量线性回归`。 

## 2. Cost Function
+ reading materials 

cost function用于评价假设函数(hypothesis function)的准确性,具体采用平方差的方式计算。在上一节中我们知道了假设函数为：
$$h_{\theta }(x)=\theta _{0}+\theta _{1}*x$$,

$Θ_i$ 是模型参数。每给出一对 $Θ_0$ 和 $Θ_1$ 的值，我们将得到一条直线拟合。我们的目标是：如何选择 $Θ_0$ 和 $Θ_1$ 得到最佳拟合直线，以使得输出变量y到假设函数 直线的垂直距离的和最小。<br>
此处我们需要使用代价函数来求出最佳的$Θ_0$ 和 $Θ_1$:
$$J(\theta _{0}, \theta _{1})=\frac{1}{2m}\sum_{i=1}^{m}(\hat{y_{i}}-y_{i})^{2}$$
其中，m是样本数量。这个函数又被称为`squared error function` 或者`mean squared error`。除以2是为了梯度下降（gradient descent）,也有利于导数项的减少。
+ 单变量代价函数

为更直观的了解代价函数的作用和功能，我们先从简化的假设函数开始。我们假设 $Θ_0$ 为0，有
$$h_{\theta }(x)=\theta _{1}*x$$,

$$J(\theta _{1})=\frac{1}{2m}\sum_{i=1}^{m}(\hat{y_{i}}-y_{i})^{^{2}}=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta }(x_{i})-y_{i})^{2}$$
现在我们开始对 $Θ_1$ 进行取值，分别画出假设函数h(x)和代价函数J(θ1)的函数。


